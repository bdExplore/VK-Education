{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8PEOM4NCCT9"
      },
      "source": [
        "# PEFT\n",
        "\n",
        "Сегодня LLM становятся незаменимым инструментом как для полноценного решения продуктовых задач, так и на промежуточных этапах, например, генерация разметки или создание синтетических датасетов. Обучение и дообучение таких моделей может быть ресурсозатратным, поэтому на зачастую полезно использовать Parameter-Efficient Fine-Tuning, PEFT.\n",
        "\n",
        "PEFT позволяет адаптировать крупные языковые модели под конкретные задачи, внося минимальные изменения в архитектуру и обучаясь на сравнительно небольшом объёме данных. Ключевые методы – такие как адаптеры, LoRA или DoRA – демонстрируют высокую эффективность, позволяя достичь конкурентоспособной точности при низких затратах на вычислительные мощности.\n",
        "\n",
        "Представьте, что ваша задача – определить тональность твитов. Твиты – короткие, насыщенные эмоциями и часто саркастичные сообщения, где традиционные модели могут давать сбои из-за неформального стиля и ограниченного контекста. Используя PEFT, вы можете донастроить LLM под особенности твиттер-лексикона, адаптируя модель на небольшом, но репрезентативном наборе данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3av_XVGoE0VB"
      },
      "source": [
        "## Импортируем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il6PTSX1dHJs"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet datasets bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k1BAjya7dXeZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, interpreter_login\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_-n5o_KE-DQH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SNdKoLWk-Be-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Homework repository: 'bdvs/llm-course-hw3'\n"
          ]
        }
      ],
      "source": [
        "# Подготовим репозиторий для будущей модели и токенизатора\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw3\"  # Или как вам хочется\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l4yiCPkk9_1I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE='mps'\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
        "# Это могут быть как целые функции, так и отдельные части внутри них\n",
        "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"{DEVICE=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6g6d07CeCMo"
      },
      "source": [
        "В качестве базовой модели возьмем [`Lite-Oute-1-300M-Instruct`](https://huggingface.co/OuteAI/Lite-Oute-1-300M-Instruct).\n",
        "Она использует за основу Mistral и насчитывает около 300 млн параметров, размер контекста до 4096 токенов.\n",
        "\n",
        "Вы можете использовать любую другую модель, однако обратите внимание, что используете на `Instruct` версию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v-ZP8x0-FWNp"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"OuteAI/Lite-Oute-1-300M-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlOV4EKtJYGo"
      },
      "source": [
        "## Подготовка данных [2 балла]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrsY38DxeCMp"
      },
      "source": [
        "Думаю, вы уже задумались над тем, что качество датасета для модели такого размера будет заметно влиять на перфоманс модели после обучения. Это действительно так, более того есть интересное исследование и на больших моделях: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) показывает, что даже большие модели (например, Llama 65B) можно успешно обучить на небольшом, но исключительно качественном наборе данных.\n",
        "\n",
        "Для нашей задачи воспользуемся стандартным датасетом классификации твиттов по тональности: [`cardiffnlp/tweet_eval`](https://huggingface.co/datasets/cardiffnlp/tweet_eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uhaucbodjoyJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text: \"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
            "label: 2\n",
            "str_label: positive\n",
            "==========\n",
            "text: \"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
            "label: 1\n",
            "str_label: neutral\n",
            "==========\n",
            "text: Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
            "label: 1\n",
            "str_label: neutral\n",
            "==========\n",
            "text: Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n",
            "label: 1\n",
            "str_label: neutral\n",
            "==========\n",
            "text: @user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"\n",
            "label: 2\n",
            "str_label: positive\n",
            "==========\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
        "\n",
        "IDX2NAME = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "\n",
        "def add_str_label(example):\n",
        "    example[\"str_label\"] = IDX2NAME[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "\n",
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(add_str_label)\n",
        "\n",
        "for i in range(5):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvrtTaU3JYGo"
      },
      "source": [
        "Для подготовки датасета необходими\n",
        "\n",
        "1. Задать системный промпт, в нем полезно описать задачу, а также определить формат генерации. В нашем случае это одно слово – название класса.\n",
        "2. Задать пользовательский промпт, в котором будет находиться текст на классификацию.\n",
        "3. Применить `chat_template` с помощью метода `tokenizer.apply_chat_template`, добавить начало генерации ассистента.\n",
        "4. Токенизировать датасет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gwGyELHEJYGo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM_PROMPT=\"Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.\"\n"
          ]
        }
      ],
      "source": [
        "# Задаем системный промпт — это инструкция для модели, как себя вести.\n",
        "SYSTEM_PROMPT = \"Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.\"\n",
        "print(f\"{SYSTEM_PROMPT=}\")\n",
        "\n",
        "\n",
        "def process_example(example, *, tokenizer, system_prompt=SYSTEM_PROMPT):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"text\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"str_label\"]},\n",
        "    ]\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"text\"]},\n",
        "        {\"role\": \"assistant\", \"content\": \"\"},\n",
        "    ]\n",
        "    example[\"prompt\"] = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "    example[\"full_prompt\"] = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iMA4x4O5joyK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text: \"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
            "label: 2\n",
            "str_label: positive\n",
            "prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "full_prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "positive<|im_end|>\n",
            "\n",
            "==========\n",
            "text: \"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
            "label: 1\n",
            "str_label: neutral\n",
            "prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "full_prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "neutral<|im_end|>\n",
            "\n",
            "==========\n"
          ]
        }
      ],
      "source": [
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(process_example, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lS8wPkiXjoyK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text: \"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
            "label: 2\n",
            "str_label: positive\n",
            "prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "full_prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "positive<|im_end|>\n",
            "\n",
            "input_ids: [1, 32001, 1587, 13, 28849, 28829, 1040, 4235, 7351, 10890, 28786, 1051, 1622, 24229, 6307, 3846, 28029, 8849, 28723, 10678, 7176, 1586, 1225, 1956, 1040, 853, 23569, 922, 2289, 4235, 2200, 20104, 10841, 853, 5954, 11249, 3882, 28795, 1621, 28723, 18998, 8496, 17483, 6565, 3678, 2289, 5524, 8895, 25255, 18653, 28747, 464, 1065, 2468, 647, 464, 23806, 1650, 28742, 10476, 464, 23238, 4135, 32000, 28705, 13, 32001, 2188, 13, 28739, 28824, 28738, 802, 1838, 560, 272, 3493, 10449, 302, 272, 28705, 28787, 362, 1820, 28725, 4561, 381, 393, 715, 262, 16761, 272, 13711, 302, 382, 476, 28727, 8060, 28723, 422, 28769, 9813, 28760, 4633, 1466, 5139, 381, 28758, 715, 262, 28739, 32000, 28705, 13, 32001, 13892, 13, 32000, 28705, 13, 32001, 13892, 13]\n",
            "full_input_ids: [1, 32001, 1587, 13, 28849, 28829, 1040, 4235, 7351, 10890, 28786, 1051, 1622, 24229, 6307, 3846, 28029, 8849, 28723, 10678, 7176, 1586, 1225, 1956, 1040, 853, 23569, 922, 2289, 4235, 2200, 20104, 10841, 853, 5954, 11249, 3882, 28795, 1621, 28723, 18998, 8496, 17483, 6565, 3678, 2289, 5524, 8895, 25255, 18653, 28747, 464, 1065, 2468, 647, 464, 23806, 1650, 28742, 10476, 464, 23238, 4135, 32000, 28705, 13, 32001, 2188, 13, 28739, 28824, 28738, 802, 1838, 560, 272, 3493, 10449, 302, 272, 28705, 28787, 362, 1820, 28725, 4561, 381, 393, 715, 262, 16761, 272, 13711, 302, 382, 476, 28727, 8060, 28723, 422, 28769, 9813, 28760, 4633, 1466, 5139, 381, 28758, 715, 262, 28739, 32000, 28705, 13, 32001, 13892, 13, 1065, 2468, 32000, 28705, 13]\n",
            "==========\n",
            "text: \"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
            "label: 1\n",
            "str_label: neutral\n",
            "prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "full_prompt: <|im_start|>system\n",
            "Вы — эксперт по анализу тональности. Ваша задача — определить эмоциональную окраску текста. Ответ должен быть одним словом: 'positive', 'neutral' или 'negative'.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "neutral<|im_end|>\n",
            "\n",
            "input_ids: [1, 32001, 1587, 13, 28849, 28829, 1040, 4235, 7351, 10890, 28786, 1051, 1622, 24229, 6307, 3846, 28029, 8849, 28723, 10678, 7176, 1586, 1225, 1956, 1040, 853, 23569, 922, 2289, 4235, 2200, 20104, 10841, 853, 5954, 11249, 3882, 28795, 1621, 28723, 18998, 8496, 17483, 6565, 3678, 2289, 5524, 8895, 25255, 18653, 28747, 464, 1065, 2468, 647, 464, 23806, 1650, 28742, 10476, 464, 23238, 4135, 32000, 28705, 13, 32001, 2188, 13, 28739, 18113, 6717, 732, 6717, 325, 514, 18637, 28731, 7520, 575, 302, 272, 1407, 715, 10983, 28725, 22050, 278, 422, 28759, 11758, 422, 28735, 28798, 28739, 32000, 28705, 13, 32001, 13892, 13, 32000, 28705, 13, 32001, 13892, 13]\n",
            "full_input_ids: [1, 32001, 1587, 13, 28849, 28829, 1040, 4235, 7351, 10890, 28786, 1051, 1622, 24229, 6307, 3846, 28029, 8849, 28723, 10678, 7176, 1586, 1225, 1956, 1040, 853, 23569, 922, 2289, 4235, 2200, 20104, 10841, 853, 5954, 11249, 3882, 28795, 1621, 28723, 18998, 8496, 17483, 6565, 3678, 2289, 5524, 8895, 25255, 18653, 28747, 464, 1065, 2468, 647, 464, 23806, 1650, 28742, 10476, 464, 23238, 4135, 32000, 28705, 13, 32001, 2188, 13, 28739, 18113, 6717, 732, 6717, 325, 514, 18637, 28731, 7520, 575, 302, 272, 1407, 715, 10983, 28725, 22050, 278, 422, 28759, 11758, 422, 28735, 28798, 28739, 32000, 28705, 13, 32001, 13892, 13, 23806, 1650, 32000, 28705, 13]\n",
            "==========\n"
          ]
        }
      ],
      "source": [
        "def tokenization(example, *, tokenizer, max_length=256):\n",
        "    example[\"input_ids\"] = tokenizer(example[\"prompt\"], max_length=max_length, truncation=True).input_ids\n",
        "    example[\"full_input_ids\"] = tokenizer(example[\"full_prompt\"], max_length=max_length, truncation=True).input_ids\n",
        "    return example\n",
        "\n",
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(tokenization, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iujjPbgZJYGp"
      },
      "source": [
        "Попробуем решить задачу исходной моделью!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tDY0RXhlB07s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
            "positive\n",
            "\"QT @user In the original draft of the 7th book,\n",
            "==========\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
            "neutral\n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday\n",
            "==========\n",
            "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
            "neutral\n",
            "The stream last night was a bit of a disappointment, as it was a bit\n",
            "==========\n",
            "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n",
            "neutral\n",
            "The game was a 8-inning affair, with both teams having a\n",
            "==========\n",
            "@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"\n",
            "positive\n",
            "The statement \"Alciato will invest 150 million in January,\n",
            "==========\n"
          ]
        }
      ],
      "source": [
        "def generate_class(model, tokenizer, input_ids):\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=16)\n",
        "    generated_text = tokenizer.decode(output_ids[0][len(input_ids[0]) :], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0Uj4riJYGp"
      },
      "source": [
        "Если вам повезло, после слова `assistant` вы увидите определение сентиментальности входящего текста. Однако, скорее всего, модель не обучена возвращать исключительно название тональности, поэтому необходим постпроцессинг ответа. Рассмотрим самый простой способ его реализации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTxJZ3ZgJYGp"
      },
      "source": [
        "## Bonus: постпроцессинг [1 балл]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8zHFD3JYGp"
      },
      "source": [
        "Ниже приведена базовая реализация функции для выделения сентиментальности текста. Однако текущая реализация имеет несколько недостатков: она может не учитывать нестандартное форматирование ответа модели, дополнительные символы или ошибки пунктуации. У вас есть возможность улучшить эту функцию, обосновать выявленные ограничения и предложить более продвинутую версию, способную корректно обрабатывать различные варианты вывода модели.\n",
        "\n",
        "Если вы не хотите в этом копаться, то просто пропустите задание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QwFX6rtOyx0A"
      },
      "outputs": [],
      "source": [
        "def postprocess_sentiment(output_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the sentiment classification (\"positive\" or \"negative\") from the model's output text.\n",
        "\n",
        "    Process:\n",
        "        1. Splits the output at the first occurrence of the keyword \"assistant\" and processes the text after it.\n",
        "        2. Uses a regular expression to search for the first occurrence of the words \"positive\" or \"negative\" (ignoring case).\n",
        "        3. Returns the found sentiment in lowercase. If no match is found, returns an empty string.\n",
        "\n",
        "    Parameters:\n",
        "        output_text (str): The complete text output from the model, including conversation headers.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentiment classification or empty string\n",
        "    \"\"\"\n",
        "\n",
        "    parts = output_text.split(\"assistant\", 1)\n",
        "    text_to_process = parts[1] if len(parts) > 1 else output_text\n",
        "\n",
        "    match = re.search(rf\"\\b({'|'.join(IDX2NAME.values())})\\b\", text_to_process, re.IGNORECASE)\n",
        "    return match.group(1).lower() if match else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "65agkcwRJYGp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'neutral'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postprocess_sentiment(\"This text is neutral, not positive or negative\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv41I7wEJYGp"
      },
      "source": [
        "## Оценка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u90EX9_aJYGp"
      },
      "source": [
        "Давайте оценим качество нашей модели. Для этого напишем функцию `eval`, которая принимает модель и датасет, генерирует для каждого примера предсказание и вычисляет точность классификации, сравнивая полученные результаты с истинными метками. Построим матрицу ошибок и посчитаем f1 для каждого класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP0dtwuarVIX"
      },
      "outputs": [],
      "source": [
        "def pad(tensors: list[torch.Tensor], padding_value: int = 0, padding_side: str = \"left\") -> torch.Tensor:\n",
        "    max_len = max(t.size(0) for t in tensors)\n",
        "    padded = []\n",
        "    for t in tensors:\n",
        "        pad_size = max_len - t.size(0)\n",
        "        if padding_side == \"left\":\n",
        "            padded_tensor = torch.cat([torch.full((pad_size,), padding_value, dtype=t.dtype, device=t.device), t])\n",
        "        else:\n",
        "            padded_tensor = torch.cat([t, torch.full((pad_size,), padding_value, dtype=t.dtype, device=t.device)])\n",
        "        padded.append(padded_tensor)\n",
        "    return torch.stack(padded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pAZcUQoJYGp"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode\n",
        "def eval(model, dataset, tokenizer, show_conf_m=True, batch_size=1):\n",
        "    \"\"\"Evaluates the given model on the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The language model used for generating sentiment predictions.\n",
        "        dataset: An iterable collection of examples, where each example is a dict with keys:\n",
        "            - \"input_ids\": The input text message.\n",
        "            - \"str_label\": The ground truth sentiment label (e.g., \"positive\" or \"negative\").\n",
        "\n",
        "    Returns:\n",
        "        float: The macro f1 score\n",
        "    \"\"\"\n",
        "    name2idx = {v: k for k, v in IDX2NAME.items()}\n",
        "    name2idx[\"\"] = len(name2idx)\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "\n",
        "    for examples in tqdm(dataset.batch(batch_size)):\n",
        "        input_ids = pad(list(map(torch.tensor, examples[\"input_ids\"])), padding_value=tokenizer.pad_token_id).to(DEVICE)\n",
        "        attention_mask = pad(list(map(lambda it: torch.ones(len(it)), examples[\"input_ids\"])), padding_value=0).to(\n",
        "            DEVICE\n",
        "        )\n",
        "        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=16)\n",
        "        shrinked_ids = output_ids[:, input_ids.shape[1] :]\n",
        "        texts = tokenizer.batch_decode(shrinked_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        for i in range(len(examples[\"str_label\"])):\n",
        "            predicted_sentiment = postprocess_sentiment(texts[i])\n",
        "            ground_truth.append(name2idx[examples[\"str_label\"][i]])\n",
        "            predicted.append(name2idx[predicted_sentiment])\n",
        "\n",
        "    if show_conf_m:\n",
        "        conf_m = confusion_matrix(ground_truth, predicted, labels=list(name2idx.values()))\n",
        "        disp = ConfusionMatrixDisplay(conf_m, display_labels=list(name2idx.keys()))\n",
        "        disp.plot()\n",
        "\n",
        "    f1 = f1_score(ground_truth, predicted, labels=list(name2idx.values()), average=\"macro\", zero_division=0.0)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PWl_vqvn7qL"
      },
      "outputs": [],
      "source": [
        "initial_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Initial Macro F1: {initial_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjP2PLRWtofz"
      },
      "source": [
        "# LoRA: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECs_d4pzeCMo"
      },
      "source": [
        "В традиционном fine-tuning больших языковых моделей требуется обновление огромного числа параметров, что приводит к высоким вычислительным затратам и потреблению памяти. Метод LoRA решает эту проблему, вводя низкоранговые обновления весов.\n",
        "\n",
        "Пусть $(W_0 \\in \\mathbb{R}^{d \\times k})$ - исходная матрица весов модели. При адаптации модели предполагается, что обновление весов можно аппроксимировать матрицей низкого ранга:\n",
        "$$\n",
        "\\Delta W = BA, \\quad B \\in \\mathbb{R}^{d \\times r}, \\quad A \\in \\mathbb{R}^{r \\times k},\n",
        "$$\n",
        "где $r \\ll \\min(d, k)$.\n",
        "\n",
        "Обновлённая матрица весов записывается как:\n",
        "$$\n",
        "W = W_0 + \\Delta W = W_0 + B A.\n",
        "$$\n",
        "\n",
        "Основная идея заключается в том, чтобы заморозить исходные параметры $W_0$ и обучать только матрицы $A$ и $B$. Это существенно уменьшает число обучаемых параметров, так как их суммарное количество равно $r \\times (d+k)$ по сравнению с $d \\times k$ для полной матрицы $W$.\n",
        "\n",
        "Матрицу $A$ рекомендуется инициализировать нормальным распределением $N(0, \\frac{1}{\\sqrt{r}})$, а матрицу $B$ нулями. Также не забывайте про скейлинг $\\Delta Wx$ на $\\frac{\\alpha}{r}$, где $\\alpha$ гипер-параметр. Во время обучения его можно зафиксировать $\\alpha=16$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6TIqiboJYGp"
      },
      "source": [
        "## Адаптиация модели с LoRA слоями [4 балла]\n",
        "\n",
        "В этом задачнии вам потребуется:\n",
        "1. Дописать класс `LoRALayer` который заменит слои модели\n",
        "2. Дописать функцию рекурсивного обхода модели, чтобы применить к ней `LinearWithLoRA`\n",
        "3. Обновить модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QuGAjHP_tof0"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        self.B = nn.Parameter(torch.zeros(in_dim, rank))  # init zeros\n",
        "        self.A = nn.Parameter(torch.randn(rank, out_dim) * (1 / (rank ** 0.5)))  # init normal\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Обеспечим, что параметры на том же устройстве, что и вход\n",
        "        B = self.B\n",
        "        A = self.A\n",
        "        if B.device != x.device:\n",
        "            B = B.to(x.device)\n",
        "        if A.device != x.device:\n",
        "            A = A.to(x.device)\n",
        "        return (x @ B @ A) * self.scaling\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p62AfaNDeFsd"
      },
      "outputs": [],
      "source": [
        "def apply_peft_to_module(model, adapter_class, r, alpha, target_submodules):\n",
        "    for name, module in model.named_children():\n",
        "        if any(target in name for target in target_submodules) and isinstance(module, nn.Linear):\n",
        "            wrapped = adapter_class(module, r, alpha)\n",
        "            setattr(model, name, wrapped)\n",
        "        else:\n",
        "            apply_peft_to_module(module, adapter_class, r, alpha, target_submodules)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSM6Sf1leCMp"
      },
      "source": [
        "### Применим наш LoRA adapter к нашей модели\n",
        "\n",
        "Обычно для дешевого обучения достаточно применить LoRA к слоям для ключей `k_proj` и значений `v_proj`. Однако, если вы уверены в своих действиях, то не воспрещается обучать и другие слои с помощью LoRA :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mO0GEFD9eCMp"
      },
      "outputs": [],
      "source": [
        "# Примените peft к модели\n",
        "apply_peft_to_module(model, LinearWithLoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T5dI0p3Ttof2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model.embed_tokens.weight: False\n",
            "model.layers.0.self_attn.q_proj.weight: False\n",
            "model.layers.0.self_attn.k_proj.linear.weight: False\n",
            "model.layers.0.self_attn.k_proj.lora.B: True\n",
            "model.layers.0.self_attn.k_proj.lora.A: True\n",
            "model.layers.0.self_attn.v_proj.linear.weight: False\n",
            "model.layers.0.self_attn.v_proj.lora.B: True\n",
            "model.layers.0.self_attn.v_proj.lora.A: True\n",
            "model.layers.0.self_attn.o_proj.weight: False\n",
            "model.layers.0.mlp.gate_proj.weight: False\n",
            "model.layers.0.mlp.up_proj.weight: False\n",
            "model.layers.0.mlp.down_proj.weight: False\n",
            "model.layers.0.input_layernorm.weight: False\n",
            "model.layers.0.post_attention_layernorm.weight: False\n",
            "model.layers.1.self_attn.q_proj.weight: False\n",
            "model.layers.1.self_attn.k_proj.linear.weight: False\n",
            "model.layers.1.self_attn.k_proj.lora.B: True\n",
            "model.layers.1.self_attn.k_proj.lora.A: True\n",
            "model.layers.1.self_attn.v_proj.linear.weight: False\n",
            "model.layers.1.self_attn.v_proj.lora.B: True\n",
            "model.layers.1.self_attn.v_proj.lora.A: True\n",
            "model.layers.1.self_attn.o_proj.weight: False\n",
            "model.layers.1.mlp.gate_proj.weight: False\n",
            "model.layers.1.mlp.up_proj.weight: False\n",
            "model.layers.1.mlp.down_proj.weight: False\n",
            "model.layers.1.input_layernorm.weight: False\n",
            "model.layers.1.post_attention_layernorm.weight: False\n",
            "model.layers.2.self_attn.q_proj.weight: False\n",
            "model.layers.2.self_attn.k_proj.linear.weight: False\n",
            "model.layers.2.self_attn.k_proj.lora.B: True\n",
            "model.layers.2.self_attn.k_proj.lora.A: True\n",
            "model.layers.2.self_attn.v_proj.linear.weight: False\n",
            "model.layers.2.self_attn.v_proj.lora.B: True\n",
            "model.layers.2.self_attn.v_proj.lora.A: True\n",
            "model.layers.2.self_attn.o_proj.weight: False\n",
            "model.layers.2.mlp.gate_proj.weight: False\n",
            "model.layers.2.mlp.up_proj.weight: False\n",
            "model.layers.2.mlp.down_proj.weight: False\n",
            "model.layers.2.input_layernorm.weight: False\n",
            "model.layers.2.post_attention_layernorm.weight: False\n",
            "model.layers.3.self_attn.q_proj.weight: False\n",
            "model.layers.3.self_attn.k_proj.linear.weight: False\n",
            "model.layers.3.self_attn.k_proj.lora.B: True\n",
            "model.layers.3.self_attn.k_proj.lora.A: True\n",
            "model.layers.3.self_attn.v_proj.linear.weight: False\n",
            "model.layers.3.self_attn.v_proj.lora.B: True\n",
            "model.layers.3.self_attn.v_proj.lora.A: True\n",
            "model.layers.3.self_attn.o_proj.weight: False\n",
            "model.layers.3.mlp.gate_proj.weight: False\n",
            "model.layers.3.mlp.up_proj.weight: False\n",
            "model.layers.3.mlp.down_proj.weight: False\n",
            "model.layers.3.input_layernorm.weight: False\n",
            "model.layers.3.post_attention_layernorm.weight: False\n",
            "model.layers.4.self_attn.q_proj.weight: False\n",
            "model.layers.4.self_attn.k_proj.linear.weight: False\n",
            "model.layers.4.self_attn.k_proj.lora.B: True\n",
            "model.layers.4.self_attn.k_proj.lora.A: True\n",
            "model.layers.4.self_attn.v_proj.linear.weight: False\n",
            "model.layers.4.self_attn.v_proj.lora.B: True\n",
            "model.layers.4.self_attn.v_proj.lora.A: True\n",
            "model.layers.4.self_attn.o_proj.weight: False\n",
            "model.layers.4.mlp.gate_proj.weight: False\n",
            "model.layers.4.mlp.up_proj.weight: False\n",
            "model.layers.4.mlp.down_proj.weight: False\n",
            "model.layers.4.input_layernorm.weight: False\n",
            "model.layers.4.post_attention_layernorm.weight: False\n",
            "model.layers.5.self_attn.q_proj.weight: False\n",
            "model.layers.5.self_attn.k_proj.linear.weight: False\n",
            "model.layers.5.self_attn.k_proj.lora.B: True\n",
            "model.layers.5.self_attn.k_proj.lora.A: True\n",
            "model.layers.5.self_attn.v_proj.linear.weight: False\n",
            "model.layers.5.self_attn.v_proj.lora.B: True\n",
            "model.layers.5.self_attn.v_proj.lora.A: True\n",
            "model.layers.5.self_attn.o_proj.weight: False\n",
            "model.layers.5.mlp.gate_proj.weight: False\n",
            "model.layers.5.mlp.up_proj.weight: False\n",
            "model.layers.5.mlp.down_proj.weight: False\n",
            "model.layers.5.input_layernorm.weight: False\n",
            "model.layers.5.post_attention_layernorm.weight: False\n",
            "model.layers.6.self_attn.q_proj.weight: False\n",
            "model.layers.6.self_attn.k_proj.linear.weight: False\n",
            "model.layers.6.self_attn.k_proj.lora.B: True\n",
            "model.layers.6.self_attn.k_proj.lora.A: True\n",
            "model.layers.6.self_attn.v_proj.linear.weight: False\n",
            "model.layers.6.self_attn.v_proj.lora.B: True\n",
            "model.layers.6.self_attn.v_proj.lora.A: True\n",
            "model.layers.6.self_attn.o_proj.weight: False\n",
            "model.layers.6.mlp.gate_proj.weight: False\n",
            "model.layers.6.mlp.up_proj.weight: False\n",
            "model.layers.6.mlp.down_proj.weight: False\n",
            "model.layers.6.input_layernorm.weight: False\n",
            "model.layers.6.post_attention_layernorm.weight: False\n",
            "model.layers.7.self_attn.q_proj.weight: False\n",
            "model.layers.7.self_attn.k_proj.linear.weight: False\n",
            "model.layers.7.self_attn.k_proj.lora.B: True\n",
            "model.layers.7.self_attn.k_proj.lora.A: True\n",
            "model.layers.7.self_attn.v_proj.linear.weight: False\n",
            "model.layers.7.self_attn.v_proj.lora.B: True\n",
            "model.layers.7.self_attn.v_proj.lora.A: True\n",
            "model.layers.7.self_attn.o_proj.weight: False\n",
            "model.layers.7.mlp.gate_proj.weight: False\n",
            "model.layers.7.mlp.up_proj.weight: False\n",
            "model.layers.7.mlp.down_proj.weight: False\n",
            "model.layers.7.input_layernorm.weight: False\n",
            "model.layers.7.post_attention_layernorm.weight: False\n",
            "model.layers.8.self_attn.q_proj.weight: False\n",
            "model.layers.8.self_attn.k_proj.linear.weight: False\n",
            "model.layers.8.self_attn.k_proj.lora.B: True\n",
            "model.layers.8.self_attn.k_proj.lora.A: True\n",
            "model.layers.8.self_attn.v_proj.linear.weight: False\n",
            "model.layers.8.self_attn.v_proj.lora.B: True\n",
            "model.layers.8.self_attn.v_proj.lora.A: True\n",
            "model.layers.8.self_attn.o_proj.weight: False\n",
            "model.layers.8.mlp.gate_proj.weight: False\n",
            "model.layers.8.mlp.up_proj.weight: False\n",
            "model.layers.8.mlp.down_proj.weight: False\n",
            "model.layers.8.input_layernorm.weight: False\n",
            "model.layers.8.post_attention_layernorm.weight: False\n",
            "model.layers.9.self_attn.q_proj.weight: False\n",
            "model.layers.9.self_attn.k_proj.linear.weight: False\n",
            "model.layers.9.self_attn.k_proj.lora.B: True\n",
            "model.layers.9.self_attn.k_proj.lora.A: True\n",
            "model.layers.9.self_attn.v_proj.linear.weight: False\n",
            "model.layers.9.self_attn.v_proj.lora.B: True\n",
            "model.layers.9.self_attn.v_proj.lora.A: True\n",
            "model.layers.9.self_attn.o_proj.weight: False\n",
            "model.layers.9.mlp.gate_proj.weight: False\n",
            "model.layers.9.mlp.up_proj.weight: False\n",
            "model.layers.9.mlp.down_proj.weight: False\n",
            "model.layers.9.input_layernorm.weight: False\n",
            "model.layers.9.post_attention_layernorm.weight: False\n",
            "model.layers.10.self_attn.q_proj.weight: False\n",
            "model.layers.10.self_attn.k_proj.linear.weight: False\n",
            "model.layers.10.self_attn.k_proj.lora.B: True\n",
            "model.layers.10.self_attn.k_proj.lora.A: True\n",
            "model.layers.10.self_attn.v_proj.linear.weight: False\n",
            "model.layers.10.self_attn.v_proj.lora.B: True\n",
            "model.layers.10.self_attn.v_proj.lora.A: True\n",
            "model.layers.10.self_attn.o_proj.weight: False\n",
            "model.layers.10.mlp.gate_proj.weight: False\n",
            "model.layers.10.mlp.up_proj.weight: False\n",
            "model.layers.10.mlp.down_proj.weight: False\n",
            "model.layers.10.input_layernorm.weight: False\n",
            "model.layers.10.post_attention_layernorm.weight: False\n",
            "model.layers.11.self_attn.q_proj.weight: False\n",
            "model.layers.11.self_attn.k_proj.linear.weight: False\n",
            "model.layers.11.self_attn.k_proj.lora.B: True\n",
            "model.layers.11.self_attn.k_proj.lora.A: True\n",
            "model.layers.11.self_attn.v_proj.linear.weight: False\n",
            "model.layers.11.self_attn.v_proj.lora.B: True\n",
            "model.layers.11.self_attn.v_proj.lora.A: True\n",
            "model.layers.11.self_attn.o_proj.weight: False\n",
            "model.layers.11.mlp.gate_proj.weight: False\n",
            "model.layers.11.mlp.up_proj.weight: False\n",
            "model.layers.11.mlp.down_proj.weight: False\n",
            "model.layers.11.input_layernorm.weight: False\n",
            "model.layers.11.post_attention_layernorm.weight: False\n",
            "model.layers.12.self_attn.q_proj.weight: False\n",
            "model.layers.12.self_attn.k_proj.linear.weight: False\n",
            "model.layers.12.self_attn.k_proj.lora.B: True\n",
            "model.layers.12.self_attn.k_proj.lora.A: True\n",
            "model.layers.12.self_attn.v_proj.linear.weight: False\n",
            "model.layers.12.self_attn.v_proj.lora.B: True\n",
            "model.layers.12.self_attn.v_proj.lora.A: True\n",
            "model.layers.12.self_attn.o_proj.weight: False\n",
            "model.layers.12.mlp.gate_proj.weight: False\n",
            "model.layers.12.mlp.up_proj.weight: False\n",
            "model.layers.12.mlp.down_proj.weight: False\n",
            "model.layers.12.input_layernorm.weight: False\n",
            "model.layers.12.post_attention_layernorm.weight: False\n",
            "model.layers.13.self_attn.q_proj.weight: False\n",
            "model.layers.13.self_attn.k_proj.linear.weight: False\n",
            "model.layers.13.self_attn.k_proj.lora.B: True\n",
            "model.layers.13.self_attn.k_proj.lora.A: True\n",
            "model.layers.13.self_attn.v_proj.linear.weight: False\n",
            "model.layers.13.self_attn.v_proj.lora.B: True\n",
            "model.layers.13.self_attn.v_proj.lora.A: True\n",
            "model.layers.13.self_attn.o_proj.weight: False\n",
            "model.layers.13.mlp.gate_proj.weight: False\n",
            "model.layers.13.mlp.up_proj.weight: False\n",
            "model.layers.13.mlp.down_proj.weight: False\n",
            "model.layers.13.input_layernorm.weight: False\n",
            "model.layers.13.post_attention_layernorm.weight: False\n",
            "model.layers.14.self_attn.q_proj.weight: False\n",
            "model.layers.14.self_attn.k_proj.linear.weight: False\n",
            "model.layers.14.self_attn.k_proj.lora.B: True\n",
            "model.layers.14.self_attn.k_proj.lora.A: True\n",
            "model.layers.14.self_attn.v_proj.linear.weight: False\n",
            "model.layers.14.self_attn.v_proj.lora.B: True\n",
            "model.layers.14.self_attn.v_proj.lora.A: True\n",
            "model.layers.14.self_attn.o_proj.weight: False\n",
            "model.layers.14.mlp.gate_proj.weight: False\n",
            "model.layers.14.mlp.up_proj.weight: False\n",
            "model.layers.14.mlp.down_proj.weight: False\n",
            "model.layers.14.input_layernorm.weight: False\n",
            "model.layers.14.post_attention_layernorm.weight: False\n",
            "model.layers.15.self_attn.q_proj.weight: False\n",
            "model.layers.15.self_attn.k_proj.linear.weight: False\n",
            "model.layers.15.self_attn.k_proj.lora.B: True\n",
            "model.layers.15.self_attn.k_proj.lora.A: True\n",
            "model.layers.15.self_attn.v_proj.linear.weight: False\n",
            "model.layers.15.self_attn.v_proj.lora.B: True\n",
            "model.layers.15.self_attn.v_proj.lora.A: True\n",
            "model.layers.15.self_attn.o_proj.weight: False\n",
            "model.layers.15.mlp.gate_proj.weight: False\n",
            "model.layers.15.mlp.up_proj.weight: False\n",
            "model.layers.15.mlp.down_proj.weight: False\n",
            "model.layers.15.input_layernorm.weight: False\n",
            "model.layers.15.post_attention_layernorm.weight: False\n",
            "model.layers.16.self_attn.q_proj.weight: False\n",
            "model.layers.16.self_attn.k_proj.linear.weight: False\n",
            "model.layers.16.self_attn.k_proj.lora.B: True\n",
            "model.layers.16.self_attn.k_proj.lora.A: True\n",
            "model.layers.16.self_attn.v_proj.linear.weight: False\n",
            "model.layers.16.self_attn.v_proj.lora.B: True\n",
            "model.layers.16.self_attn.v_proj.lora.A: True\n",
            "model.layers.16.self_attn.o_proj.weight: False\n",
            "model.layers.16.mlp.gate_proj.weight: False\n",
            "model.layers.16.mlp.up_proj.weight: False\n",
            "model.layers.16.mlp.down_proj.weight: False\n",
            "model.layers.16.input_layernorm.weight: False\n",
            "model.layers.16.post_attention_layernorm.weight: False\n",
            "model.layers.17.self_attn.q_proj.weight: False\n",
            "model.layers.17.self_attn.k_proj.linear.weight: False\n",
            "model.layers.17.self_attn.k_proj.lora.B: True\n",
            "model.layers.17.self_attn.k_proj.lora.A: True\n",
            "model.layers.17.self_attn.v_proj.linear.weight: False\n",
            "model.layers.17.self_attn.v_proj.lora.B: True\n",
            "model.layers.17.self_attn.v_proj.lora.A: True\n",
            "model.layers.17.self_attn.o_proj.weight: False\n",
            "model.layers.17.mlp.gate_proj.weight: False\n",
            "model.layers.17.mlp.up_proj.weight: False\n",
            "model.layers.17.mlp.down_proj.weight: False\n",
            "model.layers.17.input_layernorm.weight: False\n",
            "model.layers.17.post_attention_layernorm.weight: False\n",
            "model.layers.18.self_attn.q_proj.weight: False\n",
            "model.layers.18.self_attn.k_proj.linear.weight: False\n",
            "model.layers.18.self_attn.k_proj.lora.B: True\n",
            "model.layers.18.self_attn.k_proj.lora.A: True\n",
            "model.layers.18.self_attn.v_proj.linear.weight: False\n",
            "model.layers.18.self_attn.v_proj.lora.B: True\n",
            "model.layers.18.self_attn.v_proj.lora.A: True\n",
            "model.layers.18.self_attn.o_proj.weight: False\n",
            "model.layers.18.mlp.gate_proj.weight: False\n",
            "model.layers.18.mlp.up_proj.weight: False\n",
            "model.layers.18.mlp.down_proj.weight: False\n",
            "model.layers.18.input_layernorm.weight: False\n",
            "model.layers.18.post_attention_layernorm.weight: False\n",
            "model.layers.19.self_attn.q_proj.weight: False\n",
            "model.layers.19.self_attn.k_proj.linear.weight: False\n",
            "model.layers.19.self_attn.k_proj.lora.B: True\n",
            "model.layers.19.self_attn.k_proj.lora.A: True\n",
            "model.layers.19.self_attn.v_proj.linear.weight: False\n",
            "model.layers.19.self_attn.v_proj.lora.B: True\n",
            "model.layers.19.self_attn.v_proj.lora.A: True\n",
            "model.layers.19.self_attn.o_proj.weight: False\n",
            "model.layers.19.mlp.gate_proj.weight: False\n",
            "model.layers.19.mlp.up_proj.weight: False\n",
            "model.layers.19.mlp.down_proj.weight: False\n",
            "model.layers.19.input_layernorm.weight: False\n",
            "model.layers.19.post_attention_layernorm.weight: False\n",
            "model.norm.weight: False\n",
            "lm_head.weight: False\n",
            "Train 430080/300031872 (0.14%) parameters\n"
          ]
        }
      ],
      "source": [
        "# Заморозьте не нужные слои\n",
        "\n",
        "\n",
        "def freeze_layers(model, patterns):\n",
        "    for name, param in model.named_parameters():\n",
        "        # Только те параметры, которые содержат один из паттернов — остаются обучаемыми\n",
        "        if any(p in name for p in patterns):\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    total_params, trainable_params = 0, 0\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.requires_grad}\")\n",
        "        total_params += np.prod(param.shape)\n",
        "        if param.requires_grad:\n",
        "            trainable_params += np.prod(param.shape)\n",
        "    print(f\"Train {trainable_params}/{total_params} ({trainable_params / total_params * 100:.2f}%) parameters\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = freeze_layers(model, [\"lora\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDcZlsVuJYGq"
      },
      "source": [
        "## Обучение [2 балла]\n",
        "\n",
        "*Напутствие:*\n",
        "Пришло время приступить к обучению модели. После реализации train loop вы заслужите перерыв на 30+ минут – отличный повод с гордостью заявить, что вы заняты обучением модели. Однако длительность отдыха остаётся на ваше усмотрение. Мы рекомендуем ограничиться 2–3 эпохами файнтюна, чтобы избежать излишних вычислительных затрат и переобучения модели.\n",
        "\n",
        "Обратите внимание, что вам предстоит реализовать классический train loop на PyTorch. Здесь у вас достаточно свободы для выбора гиперпараметров (`batch_size`, `lr`, `num_epochs`). Вы можете использовать дополнительные гиперпараметры на свое усмотрение, например, для оптимизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y_ZfEtmwjoyL"
      },
      "outputs": [],
      "source": [
        "def pad_collate_fn(batch: list[dict[str, torch.Tensor]], pad_token_id: int) -> dict[str, torch.Tensor]:\n",
        "    \"\"\"Collates and pads a batch of tokenized inputs for classification task.\"\"\"\n",
        "    input_ids = [example[\"input_ids\"] for example in batch]\n",
        "    attention_masks = [[1] * len(ids) for ids in input_ids]\n",
        "    labels = [example[\"label\"] for example in batch]\n",
        "\n",
        "    def pad(sequences, padding_value):\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "        return torch.tensor([seq + [padding_value] * (max_len - len(seq)) for seq in sequences], dtype=torch.long)\n",
        "\n",
        "    input_ids_padded = pad(input_ids, pad_token_id)\n",
        "    attention_masks_padded = pad(attention_masks, 0)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"attention_mask\": attention_masks_padded,\n",
        "        \"labels\": labels_tensor\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k5EWaXx0joyL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    1, 32001,  1587,    13, 28849, 28829,  1040,  4235,  7351, 10890,\n",
              "          28786,  1051,  1622, 24229,  6307,  3846, 28029,  8849, 28723, 10678,\n",
              "           7176,  1586,  1225,  1956,  1040,   853, 23569,   922,  2289,  4235,\n",
              "           2200, 20104, 10841,   853,  5954, 11249,  3882, 28795,  1621, 28723,\n",
              "          18998,  8496, 17483,  6565,  3678,  2289,  5524,  8895, 25255, 18653,\n",
              "          28747,   464,  1065,  2468,   647,   464, 23806,  1650, 28742, 10476,\n",
              "            464, 23238,  4135, 32000, 28705,    13, 32001,  2188,    13, 28818,\n",
              "           1838, 24992, 20523,   624,   302,   272,  1722,   369,  2499, 10332,\n",
              "            286,   993,   506,  4067, 12642,  4187,   262,   356,   559, 19340,\n",
              "           6207, 32000, 28705,    13, 32001, 13892,    13, 32000, 28705,    13,\n",
              "          32001, 13892,    13, 32000, 32000, 32000],\n",
              "         [    1, 32001,  1587,    13, 28849, 28829,  1040,  4235,  7351, 10890,\n",
              "          28786,  1051,  1622, 24229,  6307,  3846, 28029,  8849, 28723, 10678,\n",
              "           7176,  1586,  1225,  1956,  1040,   853, 23569,   922,  2289,  4235,\n",
              "           2200, 20104, 10841,   853,  5954, 11249,  3882, 28795,  1621, 28723,\n",
              "          18998,  8496, 17483,  6565,  3678,  2289,  5524,  8895, 25255, 18653,\n",
              "          28747,   464,  1065,  2468,   647,   464, 23806,  1650, 28742, 10476,\n",
              "            464, 23238,  4135, 32000, 28705,    13, 32001,  2188,    13, 23653,\n",
              "            487,  8181, 28742, 28713,  3187,  3286,   264, 11933,  1294,   356,\n",
              "           7796, 10759, 28723, 20627,   736,   460,  1309, 12221,   297,   272,\n",
              "          17403,  1820,  5987, 28808, 32000, 28705,    13, 32001, 13892,    13,\n",
              "          32000, 28705,    13, 32001, 13892,    13]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
              " 'labels': tensor([0, 2])}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataloader = DataLoader(\n",
        "    dataset[\"train\"],\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kUXWpHlwjoyL"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, train_dataloader, val_dataset, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if step % 50 == 0 and step > 0:\n",
        "                avg_loss = running_loss / 50\n",
        "                print(f\"Epoch {epoch + 1}, Step {step} | Avg Loss: {avg_loss:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Для простоты вы можете пропустить eval, если он ещё не реализован\n",
        "        val_f1 = eval(model, val_dataset, tokenizer, show_conf_m=False)\n",
        "        print(f\"Epoch {epoch + 1} | Validation F1: {val_f1}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OIjehznTeOLP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for n, p in model.named_parameters() if \"lora\" in n], lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WdgCWxVSjoyP"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55d714a84fae463a885c6a692ac9bfa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/22808 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Expected input batch_size (240) to match target batch_size (2).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_dataloader, val_dataset, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m      6\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:863\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    866\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/loss/loss_utils.py:56\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m     55\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 56\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/loss/loss_utils.py:27\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfixed_cross_entropy\u001b[39m(source, target, num_items_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     29\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_items_in_batch\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (240) to match target batch_size (2)."
          ]
        }
      ],
      "source": [
        "model = train_model(model, optimizer, train_dataloader, dataset[\"validation\"], NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5feQFto27yUF"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXmr3w3eCMp"
      },
      "source": [
        "## Оценим результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B71QOPpEeCMp"
      },
      "source": [
        "Теперь увидим, как повлиял наш файнтюнинг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEd1sn3CJYGt"
      },
      "outputs": [],
      "source": [
        "after_lora_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"After LoRA Macro F1: {after_lora_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlSKN9tJYGt"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-lora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-lora\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GqXj9QeUbmv"
      },
      "outputs": [],
      "source": [
        "# Очистим память\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZLqE6lItof3"
      },
      "source": [
        "## DoRA: [Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqokyWcbeCMp"
      },
      "source": [
        "В отличие от метода LoRA, где обновление весов модели представлено в виде низкорангового произведения, метод DoRA вводит дополнительную степень гибкости за счёт применения диагональной матрицы для весового масштабирования.\n",
        "\n",
        "Пусть $W_0 \\in \\mathbb{R}^{d \\times k}$ - исходная матрица весов. В DoRA обновление весов определяется следующим образом:\n",
        "$$\n",
        "W = m \\frac{W_0 + BA}{\\|W_0 + BA \\|}\n",
        "$$\n",
        "\n",
        "Где $BA$ соответствует использованию LoRA, а $m \\in \\mathbb{R}^{k}$ обучаемый вектор."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah411RkgJYGu"
      },
      "source": [
        "## Обучение модели с помощью DoRA [8 баллов]\n",
        "\n",
        "В этом задании вам потребуется:\n",
        "1. Написать с нуля класс `LinearWithDoRA` который использует написанный ранее `LoRALayer` класс\n",
        "2. Применить его к модели\n",
        "3. Обучить модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCtym5B_Gkva"
      },
      "outputs": [],
      "source": [
        "class LinearWithDoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x):\n",
        "        todo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXowFAoeCMp"
      },
      "source": [
        "Сбрасываем модель и применяем наш DoRA адаптер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KBnzrTctof3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "apply_peft_to_module(model, LinearWithDoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])\n",
        "\n",
        "model = freeze_layers(model, [\"lora\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0zhC40OJYGu"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRkx5lW9tof3"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"].take(10_000),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for n, p in model.named_parameters() if \"lora\" in n], lr=LEARNING_RATE, weight_decay=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyxkGpb6joyP"
      },
      "outputs": [],
      "source": [
        "model = train_model(model, optimizer, train_dataloader, dataset[\"validation\"], NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVpdAGlq_UVh"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn4CRy9gJYGu"
      },
      "outputs": [],
      "source": [
        "after_dora_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"After DoRA Macro F1: {after_dora_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsw1xNriLOK1"
      },
      "source": [
        "Для качественного обучения доры в этой задаче нужно постараться.\n",
        "Будем считать, что если качество > 0.5, то задание с учетом правильности кода решено верно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iKx3V44Wtof4"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-dora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-dora\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfHn0cq2joyQ"
      },
      "outputs": [],
      "source": [
        "# Очистим память\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2oIfqq-luU"
      },
      "source": [
        "## Frameworks way [4 балла]\n",
        "\n",
        "### QLoRA: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)\n",
        "\n",
        "Метод QLoRA направлен на эффективное дообучение предварительно обученных больших языковых моделей с использованием квантования весов. Основная идея заключается в том, что во время обратного распространения ошибки модель квантует исходные веса с точностью до 4 бит, что позволяет значительно сократить использование GPU памяти. Для обработки пиков памяти при этом применяются страничные оптимизаторы.\n",
        "\n",
        "В результате, применение QLoRA часто приводит к экономии GPU памяти примерно на $\\dfrac{1}{3}$, однако время обучения при этом может увеличиться почти на $\\dfrac{1}{4}$ по сравнению с традиционными методами дообучения.\n",
        "\n",
        "Такой компромисс между экономией памяти и увеличением времени обучения делает QLoRA привлекательным решением в сценариях, где ресурсы ограничены, а эффективность использования памяти критически важна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFawKYW2JYGu"
      },
      "source": [
        "Теперь с таким прекрасным инструментом как QLoRA можем рассмотреть более тяжелую модель аж 1.1B :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORyx9BcxTeIB"
      },
      "source": [
        "Используйте документацию библиотек [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) и [PEFT](https://huggingface.co/docs/peft/index).\n",
        "\n",
        "1. Конфигурация квантизации (`BitsAndBytesConfig`): Подберите тип 4-битной квантизации (`bnb_4bit_quant_type`) и размерность подсчёта (`bnb_4bit_compute_dtype`)\n",
        "\n",
        "2. Настройка LoRA-адаптеров (`LoraConfig`):\n",
        "    - Подберите и обоснуйте значения следующих гиперпараметров:\n",
        "        - `lora_alpha`\n",
        "        - `lora_dropout`\n",
        "        - `r`\n",
        "    - Выберите модули модели, к которым следует применять LoRA-адаптеры (`target_modules`).\n",
        "\n",
        "3. Настройка параметров обучения (`TrainingArguments`, `SFTTrainer`):\n",
        "Используя документацию и подберите параметры:\n",
        "    - `learning_rate`\n",
        "    - `num_train_epochs`\n",
        "    - `gradient_accumulation_steps`\n",
        "    - `lr_scheduler_type`\n",
        "    - `per_device_train_batch_size`\n",
        "    - другие параметры по необходимости\n",
        "\n",
        "4. Проведение обучения и анализ результатов:\n",
        "    - Запустите обучение модели.\n",
        "    - Оцените модель до и после обучения.\n",
        "    - сохраните модель\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbVVMEvgeCMq"
      },
      "outputs": [],
      "source": [
        "LARGE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Заведем конфиг для квантизации\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=todo()\n",
        "    bnb_4bit_compute_dtype=todo()\n",
        ")\n",
        "\n",
        "# Инициализация квантованной модели\n",
        "model = AutoModelForCausalLM.from_pretrained(LARGE_MODEL_NAME, quantization_config=bnb_config)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3eNbrPleCMr"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LARGE_MODEL_NAME)\n",
        "tokenizer.pad_token = \"<PAD>\"\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaC7cr07KR6w"
      },
      "outputs": [],
      "source": [
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(process_example, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "    dataset[split] = data.map(tokenization, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RoZzzaJjoyQ"
      },
      "outputs": [],
      "source": [
        "initial_f1_large_model = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Initial F1 large model: {initial_f1_large_model:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvkzXT4eCMr"
      },
      "source": [
        "### Обучим QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAtG5fqeCMr"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=todo(),\n",
        "    lora_dropout=todo(),\n",
        "    r=todo(),\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=todo(),\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnekx15FOQf7"
      },
      "source": [
        "`SFTTrainer` supports conversational format:\n",
        "```\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVpClyiJObAE"
      },
      "outputs": [],
      "source": [
        "def convert_instruction_format(example, system_prompt=SYSTEM_PROMPT):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Text: {example['text']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"str_label\"]},\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "\n",
        "sft_dataset = dataset.copy()\n",
        "for split, data in dataset.items():\n",
        "    sft_dataset[split] = data.map(\n",
        "        convert_instruction_format,\n",
        "        remove_columns=[\"text\", \"label\", \"str_label\", \"prompt\", \"full_prompt\", \"input_ids\", \"full_input_ids\"],\n",
        "    )\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in sft_dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_qexatpeCMr"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(todo())\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=sft_dataset[\"train\"],\n",
        "    args=training_arguments,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZD2lveJYGv"
      },
      "outputs": [],
      "source": [
        "qlora_large_model = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"QLoRA F1 large model: {qlora_large_model:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV8cJJTyJYGv"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-tinyllama-qlora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-tinyllamma-qlora\", private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzNfBG6cfcwC"
      },
      "source": [
        "# Дополнительные баллы\n",
        "\n",
        "Вы также можно заработать дополнительные баллы:\n",
        "- Оформить репозитории на 🤗 (можно сделать коллекцию, так как у нас 3 репозитория): карточка модели с описанием задания, репортом качества и примерами генерации **[2 балла]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVxsIyFJdUC"
      },
      "source": [
        "# Специальный раздел для проверяющего"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE6NoTX3Jxp4"
      },
      "source": [
        "## LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIMzSBypJgMF"
      },
      "outputs": [],
      "source": [
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-lora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-lora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Примените peft к модели\n",
        "apply_peft_to_module(model, LinearWithLoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])\n",
        "model.to(DEVICE)\n",
        "\n",
        "path = hf_hub_download(f\"{REPO_NAME}-lora\", \"model.safetensors\")\n",
        "state_dict = load_file(path)\n",
        "\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "LoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after LoRA training: {LoRA_saved_model_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zG4EGiMJ0Um"
      },
      "source": [
        "## DoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRq41oNiJ1fi"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-dora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-dora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "apply_peft_to_module(model, LinearWithDoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])\n",
        "model.to(DEVICE)\n",
        "\n",
        "path = hf_hub_download(f\"{REPO_NAME}-dora\", \"model.safetensors\")\n",
        "state_dict = load_file(path)\n",
        "\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "DoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after DoRA training: {DoRA_saved_model_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z-4DEA6J19f"
      },
      "source": [
        "## QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU6wcevzJ3Gn"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-tinyllama-qlora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-tinyllama-qlora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "QLoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after tinyllama QLoRA training: {QLoRA_saved_model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
