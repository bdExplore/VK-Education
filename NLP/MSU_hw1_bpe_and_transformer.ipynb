{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBA5IPyb4BFH"
      },
      "source": [
        "# –ï–≥–æ –≤–µ–ª–∏—á–µ—Å—Ç–≤–æ, \"–¥–æ–º–∞—à–∫–∞ ‚Ññ1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnY6pwvpghE"
      },
      "source": [
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–∏—Ç—å Byte-level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –Ω–µ–±–æ–ª—å—à—É—é LM.  \n",
        "\n",
        "–î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Ä—É—Å—Å–∫–∏–º–∏ –∞–Ω–µ–∫–¥–æ—Ç–∞–º–∏!\n",
        "\n",
        "–û–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏ –Ω—É–∂–Ω–æ –≤—ã–ª–æ–∂–∏—Ç—å –Ω–∞ [ü§ó HuggingFace](https://huggingface.co/). –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å —Ç–∞–º, –ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ [deep vk](https://huggingface.co/deepvk) –∏ —Å–æ–∑–¥–∞–π—Ç–µ —Å–µ–±–µ API —Ç–æ–∫–µ–Ω.\n",
        "\n",
        "–°–ª–µ–¥—É–π—Ç–µ —è—á–µ–π–∫–∞–º —Ç–µ—Ç—Ä–∞–¥–∫–∏ –∏ –∑–∞–ø–æ–ª–Ω—è–π—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —è—á–µ–π–∫–∏. –í –∫–æ–Ω—Ü–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏ –≤—ã –Ω–∞–π–¥–µ—Ç–µ –∑–∞–¥–∞—á–∏ —Å–æ –∑–≤–µ–∑–¥–æ—á–∫–æ–π, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0byNYx5dzB4b"
      },
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "\n",
        "%pip install --quiet datasets livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade seleniumbase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILR1tu3z9oI"
      },
      "outputs": [],
      "source": [
        "# –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache, partial\n",
        "from pathlib import Path\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download\n",
        "from huggingface_hub.utils import SoftTemporaryDirectory\n",
        "from livelossplot import PlotLosses\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7OLSivnPW0"
      },
      "outputs": [],
      "source": [
        "# –≠—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –±—É–¥—É—Ç –ø–æ–º–µ—á–µ–Ω—ã –≤—Å–µ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–∑–∞–ø–æ–ª–Ω–∏—Ç—å\n",
        "# –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ —Ü–µ–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö\n",
        "# –í—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏–µ–π –∏ –Ω–∞–π—Ç–∏ –º–µ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ :)\n",
        "\n",
        "\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkJJp2JK99x"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVWKkwaryDTq"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –±—É–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw1\"  # –ò–ª–∏ –∫–∞–∫ –≤–∞–º —Ö–æ—á–µ—Ç—Å—è\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")\n",
        "\n",
        "# –ò –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –≤–µ—â–∏\n",
        "SEED = 0xC0FFEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxN5JUbZ3ToV"
      },
      "source": [
        "# –î–∞—Ç–∞—Å–µ—Ç\n",
        "\n",
        "–ü–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ: [ü§ó IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)\n",
        "\n",
        "–ò –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–∏—Ö üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT78JNcqpRXW"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"IgorVolochay/russian_jokes\")\n",
        "print(\"\\n===\\n\".join(dataset[\"train\"][\"text\"][:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzvdEVO3-kM"
      },
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä [6 –±–∞–ª–ª–æ–≤]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç Byte-level BPE.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ:\n",
        "1. –†–µ–∞–ª–∏–∑—É–µ–º –µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–∞–±–æ—Ä —Å–ª–∏—è–Ω–∏–π –ø–æ —ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—é\n",
        "2. –û–±—É—á–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "3. –†–µ–∞–ª–∏–∑—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7G7o6OdK99z"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ö–æ–ª–¥–∞—É—Ç—ã\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U6H1iLU3kI"
      },
      "outputs": [],
      "source": [
        "# –í—Å—è–∫–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏\n",
        "\n",
        "WHITESPACE_SPLITTER = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "def bytes_to_unicode() -> dict[int, str]:\n",
        "    \"\"\"The original dictionary consists of 256 bytes and their corresponding Unicode characters.\n",
        "    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,\n",
        "    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.\n",
        "    \"\"\"\n",
        "    initial_bytes = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n",
        "    )\n",
        "    initial_chars = [chr(it) for it in initial_bytes]\n",
        "    n = 0\n",
        "    for byte in range(2**8):\n",
        "        if byte not in initial_bytes:\n",
        "            initial_bytes.append(byte)\n",
        "            initial_chars.append(chr(2**8 + n))\n",
        "            n += 1\n",
        "    return dict(sorted(zip(initial_bytes, initial_chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge(merge_pair, pair_frequences, words_by_tokens):\n",
        "    new_token = ''.join(merge_pair)\n",
        "    new_words_by_tokens = Counter()\n",
        "    \n",
        "    for word, freq in words_by_tokens.items():\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and (word[i], word[i + 1]) == merge_pair:\n",
        "                new_word.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_words_by_tokens[tuple(new_word)] = freq\n",
        "    \n",
        "    pair_frequences = Counter()\n",
        "    for word, freq in new_words_by_tokens.items():\n",
        "        for i in range(len(word) - 1):\n",
        "            pair_frequences[(word[i], word[i + 1])] += freq\n",
        "    \n",
        "    return pair_frequences, new_words_by_tokens\n",
        "\n",
        "def train(data, vocab_size=1024, special_tokens=None):\n",
        "    if vocab_size < 256:\n",
        "        raise ValueError(\"Vocab size can't be less than 256\")\n",
        "    if special_tokens is None:\n",
        "        special_tokens = []\n",
        "    \n",
        "    id2token = bytes_to_unicode()  \n",
        "    merges = []\n",
        "    \n",
        "    words_by_tokens = Counter()\n",
        "    for sample in tqdm(data, desc=\"Loading data\"):\n",
        "        words = WHITESPACE_SPLITTER.findall(sample.strip())\n",
        "        for word in words:\n",
        "            words_by_tokens[tuple(word)] += 1\n",
        "    \n",
        "    pair_frequences = Counter()\n",
        "    for word, freq in words_by_tokens.items():\n",
        "        for i in range(len(word) - 1):\n",
        "            pair_frequences[(word[i], word[i + 1])] += freq\n",
        "    \n",
        "    pbar = trange(vocab_size, desc=\"Building vocabulary\", initial=len(id2token) + len(special_tokens))\n",
        "    while len(id2token) < vocab_size - len(special_tokens):\n",
        "        if not pair_frequences:\n",
        "            print(\"Not enough data to fulfil vocabulary\")\n",
        "            break\n",
        "        \n",
        "        top_pair = max(pair_frequences, key=pair_frequences.get)\n",
        "        new_token = ''.join(top_pair)\n",
        "        del pair_frequences[top_pair]\n",
        "        \n",
        "        if new_token in id2token.values():\n",
        "            continue\n",
        "        \n",
        "        id2token[len(id2token)] = new_token\n",
        "        merges.append(top_pair)\n",
        "        \n",
        "        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)\n",
        "        pbar.update()\n",
        "    pbar.close()\n",
        "    \n",
        "    for special_token in special_tokens:\n",
        "        id2token[len(id2token)] = special_token\n",
        "    \n",
        "    return {v: k for k, v in id2token.items()}, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLwur-KgK990"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö\n",
        "# –î–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏ —Ö–≤–∞—Ç–∏—Ç –∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –Ω–æ –º–æ–∂–µ—Ç–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∏—Ç—å!\n",
        "\n",
        "\n",
        "vocab, merges = train(dataset[\"train\"][\"text\"], vocab_size=1024, special_tokens=[\"[EOS]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsapJUAK990"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "\n",
        "random_tokens = [512, 614, 768, 888, 1022]\n",
        "unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}\n",
        "for token_id in random_tokens:\n",
        "    token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "    raw_bytes = bytes([unicode_to_bytes[it] for it in token])\n",
        "    print(f\"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUz7cma1Czs"
      },
      "outputs": [],
      "source": [
        "class ByteLevelBPETokenizer:\n",
        "    def __init__(self, vocab, merges, eos_token=\"[EOS]\"):\n",
        "        super().__init__()\n",
        "        if eos_token not in vocab:\n",
        "            raise ValueError(\"There is no EOS token in vocab\")\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.token2id = vocab\n",
        "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
        "        self.eos_token = eos_token\n",
        "        self.eos_token_id = self.token2id[eos_token]\n",
        "        self.merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "    \n",
        "    @lru_cache\n",
        "    def bpe(self, word):\n",
        "        word = tuple(word)\n",
        "        pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "        while pairs:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i + 1]) == bigram:\n",
        "                    new_word.append(''.join(bigram))\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = tuple(new_word)\n",
        "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "        return word\n",
        "    \n",
        "    def encode(self, text, add_eos_token=True):\n",
        "        words = WHITESPACE_SPLITTER.findall(text)\n",
        "        tokens = [token for word in words for token in self.bpe(word)]\n",
        "        token_ids = [self.token2id[token] for token in tokens if token in self.token2id]\n",
        "        if add_eos_token:\n",
        "            token_ids.append(self.eos_token_id)\n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, idx):\n",
        "        tokens = [self.id2token[i] for i in idx if i in self.id2token]\n",
        "        text = ''.join(tokens)\n",
        "        return text\n",
        "    \n",
        "    \n",
        "    def push_to_hub(self, repo_id, *, private=None, token=None):\n",
        "        api = HfApi()\n",
        "        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id\n",
        "\n",
        "        # Push the files to the repo in a single commit\n",
        "        with SoftTemporaryDirectory() as tmp:\n",
        "            save_directory = Path(tmp) / repo_id\n",
        "            save_directory.mkdir(parents=True)\n",
        "            with open(save_directory / \"vocabulary.json\", \"w\") as f_out:\n",
        "                print(json.dumps(self.token2id, indent=2), file=f_out)\n",
        "            with open(save_directory / \"merges.json\", \"w\") as f_out:\n",
        "                print(json.dumps({\"merges\": self.merges}), file=f_out)\n",
        "\n",
        "            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):\n",
        "        if not os.path.isdir(pretrained_model_name_or_path):\n",
        "            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)\n",
        "        else:\n",
        "            storage_folder = pretrained_model_name_or_path\n",
        "        storage_folder = Path(storage_folder)\n",
        "        with open(storage_folder / \"vocabulary.json\", \"r\") as f_in:\n",
        "            vocab = json.load(f_in)\n",
        "        with open(storage_folder / \"merges.json\", \"r\") as f_in:\n",
        "            merges = [tuple(it) for it in json.load(f_in)[\"merges\"]]\n",
        "        return cls(vocab, merges, **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTQzO3wK991"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(vocab, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9m_65vBK991"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ö–∞–±\n",
        "\n",
        "tokenizer.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9VohtKrK991"
      },
      "outputs": [],
      "source": [
        "# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ö–∞–±–∞\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvf_Q8knPW4"
      },
      "outputs": [],
      "source": [
        "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "\n",
        "text = \"–ß—Ç–æ –±—ã–ª–æ –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥? –ü–æ–º–∏–º–æ –≥—Ä–∞–Ω–¥–∏–æ–∑–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π, –ø–æ–ª–≥–æ–¥–∞ –Ω–∞–∑–∞–¥ –±—ã–ª–∏ –µ—â—ë —Å–µ–º–∏–Ω–∞—Ä—ã –ø–æ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–µ.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "reverse_text = [tokenizer.decode([it]) for it in ids]\n",
        "print(\"|\".join(reverse_text))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgpwFYiK991"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å—á–∏—Ç–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏–º—Å—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É –º–æ–¥–µ–ª–∏\n",
        "\n",
        "lens = []\n",
        "for text in tqdm(dataset[\"test\"][\"text\"]):\n",
        "    ids = tokenizer.encode(text)\n",
        "    lens.append(len(ids))\n",
        "\n",
        "print(f\"Average token len per sample: {sum(lens) / len(lens):.2f}\")\n",
        "print(f\"Minimum and maximum lens are: {min(lens)} and {max(lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwN3mfmznPW5"
      },
      "source": [
        "–î–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ 70 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.\n",
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 128 —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥–µ—Ç –≤–ø–æ–ª–Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1FljGXC7I-"
      },
      "source": [
        "# –ú–æ–¥–µ–ª—å [10 –±–∞–ª–ª–æ–≤]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª–∏–∑—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º\n",
        "1. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ALiBi\n",
        "2. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GQA\n",
        "3. –í Feed-Forward –±–ª–æ–∫–µ SwiGLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZrCvkmK992"
      },
      "outputs": [],
      "source": [
        "# –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∑–∞–≤–µ–¥–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_kv_head: int\n",
        "    hidden_dim: int\n",
        "    intermediate_dim: int\n",
        "    dropout: float = 0.1\n",
        "    vocab_size: int = 1024\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"nano\": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),\n",
        "    \"mini\": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),\n",
        "    \"small\": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYXMb5PEDFkt"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        norm_x = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        return self.scale * norm_x\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.head_dim = config.hidden_dim // config.n_head\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_per_kv = config.n_head // config.n_kv_head\n",
        "\n",
        "        self.q_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.kv_proj = nn.Linear(config.hidden_dim, 2 * config.n_kv_head * self.head_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", self._create_causal_mask(config.max_seq_len))\n",
        "        self.register_buffer(\"alibi\", self._build_alibi_bias(config.n_head))\n",
        "\n",
        "    def _build_alibi_bias(self, num_heads: int) -> Tensor:\n",
        "        slopes = torch.tensor([2**(-8 * i / num_heads) for i in range(num_heads)])\n",
        "        return slopes.view(1, num_heads, 1, 1)\n",
        "\n",
        "    def _create_causal_mask(self, max_seq_len: int) -> Tensor:\n",
        "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
        "        return mask.view(1, 1, max_seq_len, max_seq_len)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.config.n_head, self.head_dim).transpose(1, 2)\n",
        "        kv = self.kv_proj(x).view(batch_size, seq_len, 2, self.config.n_kv_head, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "        k = k.repeat_interleave(self.q_per_kv, dim=1)  # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ k, v –∫ –æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
        "        v = v.repeat_interleave(self.q_per_kv, dim=1)\n",
        "        \n",
        "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale + self.alibi\n",
        "        attn_weights = attn_weights.masked_fill(self.causal_mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        output = (attn_probs @ v).transpose(1, 2).reshape(batch_size, seq_len, self.config.hidden_dim)\n",
        "        return self.out_proj(output)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config.hidden_dim, config.hidden_dim * 2)\n",
        "        self.fc2 = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x1, x2 = self.fc1(x).chunk(2, dim=-1)\n",
        "        return self.fc2(F.silu(x1) * x2)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        x = x + self.res_dropout_1(self.attn(self.ln_1(x), attention_mask))\n",
        "        x = x + self.res_dropout_2(self.mlp(self.ln_2(x)))\n",
        "        return x\n",
        "\n",
        "class TransformerForCausalLM(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        x = self.token_emb(input_ids)\n",
        "        x = self.emb_dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        x = self.ln_final(x)\n",
        "        return self.lm_head(x)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None) -> Tensor:\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
        "            logits = self(idx_cond)[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                top_values, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < top_values[:, -1, None]] = float('-inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if do_sample else torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdavMmSDujw"
      },
      "source": [
        "# Train Loop [2 + 2 –±–∞–ª–ª–∞]\n",
        "\n",
        "–ù–∞—Å—Ç–∞–ª–æ –≤—Ä–µ–º—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å.\n",
        "–ù–µ–±–æ–ª—å—à—É—é –º–æ–∂–Ω–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å –æ–±—É—á–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è GPU, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ Google Colab.\n",
        "\n",
        "–ó–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é 2 –±–∞–ª–ª–∞, –∏ –µ—â–µ 2 –±–∞–ª–ª–∞ - –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å –≥–µ–Ω–µ—Ä–∏—Ç—å –∞–Ω–µ–∫–¥–æ—Ç—ã.\n",
        "\n",
        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏ –Ω—É–∂–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ HF –∏ —É –ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ —Å–∫–∞—á–∞–µ—Ç—Å—è –Ω—É–∂–Ω–∞—è –≤–µ—Ä—Å–∏—è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wtAiR_aDxed"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–∞–∫ –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å —Å–µ–º–ø–ª—ã –≤ –±–∞—Ç—á\n",
        "# –†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –¥–ª–∏–Ω—É, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–∞ —Å–µ–º–ø–ª–∞\n",
        "# –¢–∞–∫ –∂–µ –∑–∞–≤–µ–¥–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –º–∞—Å–∫—É, —á—Ç–æ–±—ã –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª –ø–∞–¥–∏–Ω–≥–∏\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.texts[idx]\n",
        "        tokenized_sequence = self.tokenizer.encode(texts)\n",
        "        return tokenized_sequence\n",
        "\n",
        "\n",
        "def data_collator(\n",
        "    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    batch_size = len(tokenized_sequences)\n",
        "    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)\n",
        "    attention_mask = torch.zeros((batch_size, max_batch_seq_len))\n",
        "\n",
        "    for i, tok_seq in enumerate(tokenized_sequences):\n",
        "        cur_len = min(len(tok_seq), max_batch_seq_len)\n",
        "        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])\n",
        "        attention_mask[i, :cur_len] = 1\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):\n",
        "    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "_d = TextDataset([\"–ü—Ä–∏–≤–µ—Ç!\", \"–ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?\", \"–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–æ –∫–æ–Ω—Ü–∞\"], tokenizer)\n",
        "_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)\n",
        "\n",
        "for i, batch in enumerate(_dl):\n",
        "    print(f\"Batch #{i}\")\n",
        "    input_ids, attn_mask = batch\n",
        "    print(input_ids, attn_mask, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i719AOdQK993"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "\n",
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Scheduler for Optimizer with linear warmup and linear decay to the end of training\n",
        "\n",
        "    Args:\n",
        "        optimizer: torch optimizer to control learning rate\n",
        "        num_warmup_steps: number of warmup steps\n",
        "        num_training_steps: total number of training steps\n",
        "    Return:\n",
        "        torch learning rate scheduler\n",
        "    \"\"\"\n",
        "    assert num_training_steps >= num_warmup_steps\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:\n",
        "    \"\"\"Calculate Cross-Entropy loss for Language Modeling task\n",
        "    Under the hood:\n",
        "    1. Create targets based on input ids\n",
        "    2. Masked out tokens corresponded to paddings\n",
        "    3. Calculate cross entropy loss\n",
        "\n",
        "    Args:\n",
        "        input_ids: tensor with input ids, shape [bs, seq len]\n",
        "        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]\n",
        "        logits: predicted logits, shape [bs, seq len, vocab size]\n",
        "    Return:\n",
        "        cross entropy loss, single-item tensor\n",
        "    \"\"\"\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = input_ids[:, 1:].contiguous()\n",
        "    shift_mask = attention_mask[:, 1:].contiguous()\n",
        "    \n",
        "    loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    loss = loss.view(shift_labels.size())\n",
        "    loss = loss * shift_mask\n",
        "    return loss.sum() / shift_mask.sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYdF52zXtoX"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        clip_grad_norm=1.0,\n",
        "        n_steps=10_000,\n",
        "        val_every_n_steps=1_000,\n",
        "        plot_every_n_steps=100,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.n_steps = n_steps\n",
        "        self.val_every_n_steps = val_every_n_steps\n",
        "        self.plot_every_n_steps = plot_every_n_steps\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, val_loader):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def run(self, model, train_loader, val_loader):\n",
        "        model = model.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "        plotlosses = PlotLosses(figsize=(15, 9), step_names=\"Step\")\n",
        "        logs = {\"lr\": 0, \"epoch\": 0}\n",
        "\n",
        "        data_iter = iter(train_loader)\n",
        "        for iter_num in range(self.n_steps):\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                logs[\"epoch\"] += 1\n",
        "                batch = next(data_iter)\n",
        "\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            loss = cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:\n",
        "                val_loss = self.validate(model, val_loader)\n",
        "                plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % self.plot_every_n_steps == 0:\n",
        "                logs[\"loss\"] = loss.item()\n",
        "                logs[\"lr\"] = scheduler.get_last_lr()[0]\n",
        "                plotlosses.update(logs, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "\n",
        "        val_loss = self.validate(model, val_loader)\n",
        "        plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "        plotlosses.send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1BpJflMTAi"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = TextDataset(dataset[\"train\"][\"text\"], tokenizer)\n",
        "train_dataloader = create_dataloader(\n",
        "    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(dataset[\"test\"][\"text\"], tokenizer)\n",
        "test_dataloader = create_dataloader(\n",
        "    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jHSgMZECGl"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
        "\n",
        "config = model_configs[\"nano\"]\n",
        "model = TransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUsjHl4Nkoa"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞\n",
        "\n",
        "trainer = Trainer(learning_rate=9e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUgUfpKK993"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ goes brrrr!\n",
        "\n",
        "trainer.run(model, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94zNDLEqdQow"
      },
      "outputs": [],
      "source": [
        "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–ª–∞–∑–∞–º–∏\n",
        "# –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∏ —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π \"–∑–∞—Ç—è–≥–∏–≤–∞–µ–º\" –≥–∞–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "\n",
        "text = \"–ó–∞—Ö–æ–¥–∏—Ç –≤ –±–∞—Ä\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]\n",
        "print(input_ids)\n",
        "model_output = model.generate(\n",
        "    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFi7M9ExHWv9"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±\n",
        "\n",
        "model.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrMwV9zK993"
      },
      "source": [
        "–ü–æ–∏–≥—Ä–∞–π—Ç–µ—Å—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å `mini` –∏ `small` –≤–µ—Ä—Å–∏–∏.\n",
        "–ü–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –¥–æ–±–∏—Ç—å—Å—è –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–∫ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –ª–æ—Å—Å–∞, —Ç–∞–∫ –∏ –ø—Ä–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã\n",
        "\n",
        "–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã:\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Rotary Positional Embedding **[4 –±–∞–ª–ª–∞]**\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Multi-Head Latent Attention **[2 –±–∞–ª–ª]**\n",
        "- –û—Ñ–æ—Ä–º–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ ü§ó: –∫–∞—Ä—Ç–æ—á–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∑–∞–¥–∞–Ω–∏—è, —Ä–µ–ø–æ—Ä—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **[2 –±–∞–ª–ª]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8bIDy5dKXM"
      },
      "source": [
        "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZplshN5HtRb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)\n",
        "check_model = TransformerForCausalLM.from_pretrained(REPO_NAME)\n",
        "check_model = check_model.to(device)\n",
        "check_model = check_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araF_3noK994"
      },
      "outputs": [],
      "source": [
        "text = \"–®—Ç–∏—Ä–ª–∏—Ü –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text), device=device)\n",
        "model_output = check_model.generate(\n",
        "    input_ids[None, :], max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
