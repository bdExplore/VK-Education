{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBA5IPyb4BFH"
      },
      "source": [
        "# Его величество, \"домашка №1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnY6pwvpghE"
      },
      "source": [
        "В этой домашней работе вам предоставится уникальная возможность обучить Byte-level BPE токенизатор и небольшую LM.  \n",
        "\n",
        "Домашняя работа состоит из нескольких последовательных блоков: реализация и обучение токенизатора, реализация Transformer модели и обучение модели на датасете с русскими анекдотами!\n",
        "\n",
        "Обученные токенизатор и модель можно и нужно выложить на [🤗 HuggingFace](https://huggingface.co/). Зарегистрируйтесь там, подпишитесь на [deep vk](https://huggingface.co/deepvk) и создайте себе API токен.\n",
        "\n",
        "Следуйте ячейкам тетрадки и заполняйте пропущенные ячейки. В конце тетрадки вы найдете задачи со звездочкой, чтобы получить максимальный балл!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0byNYx5dzB4b"
      },
      "outputs": [],
      "source": [
        "# Установим необходимые дополнительные библиотеки\n",
        "\n",
        "%pip install --quiet datasets livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade seleniumbase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILR1tu3z9oI"
      },
      "outputs": [],
      "source": [
        "# Необходимые импорты\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache, partial\n",
        "from pathlib import Path\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download\n",
        "from huggingface_hub.utils import SoftTemporaryDirectory\n",
        "from livelossplot import PlotLosses\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7OLSivnPW0"
      },
      "outputs": [],
      "source": [
        "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
        "# Это могут быть как целые функции, так и отдельные части внутри них\n",
        "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
        "\n",
        "\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkJJp2JK99x"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVWKkwaryDTq"
      },
      "outputs": [],
      "source": [
        "# Подготовим репозиторий для будущей модели и токенизатора\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw1\"  # Или как вам хочется\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")\n",
        "\n",
        "# И другие полезные вещи\n",
        "SEED = 0xC0FFEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxN5JUbZ3ToV"
      },
      "source": [
        "# Датасет\n",
        "\n",
        "Первым делом загрузим данные: [🤗 IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)\n",
        "\n",
        "И немного посмотрим на них 👀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT78JNcqpRXW"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"IgorVolochay/russian_jokes\")\n",
        "print(\"\\n===\\n\".join(dataset[\"train\"][\"text\"][:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzvdEVO3-kM"
      },
      "source": [
        "# Токенизатор [6 баллов]\n",
        "\n",
        "В качестве токенизатора будем использоват Byte-level BPE.\n",
        "\n",
        "Для этого:\n",
        "1. Реализуем его обучения, нам необходимо построить словарь заданного размера и набор слияний по этому словарю\n",
        "2. Обучим токенизатор на датасете\n",
        "3. Реализуем инференс токенизатора: кодирование текста и декодирование токенов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7G7o6OdK99z"
      },
      "outputs": [],
      "source": [
        "# Подготовим холдауты\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U6H1iLU3kI"
      },
      "outputs": [],
      "source": [
        "# Всякие полезности\n",
        "\n",
        "WHITESPACE_SPLITTER = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "def bytes_to_unicode() -> dict[int, str]:\n",
        "    \"\"\"The original dictionary consists of 256 bytes and their corresponding Unicode characters.\n",
        "    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,\n",
        "    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.\n",
        "    \"\"\"\n",
        "    initial_bytes = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    )\n",
        "    initial_chars = [chr(it) for it in initial_bytes]\n",
        "    n = 0\n",
        "    for byte in range(2**8):\n",
        "        if byte not in initial_bytes:\n",
        "            initial_bytes.append(byte)\n",
        "            initial_chars.append(chr(2**8 + n))\n",
        "            n += 1\n",
        "    return dict(sorted(zip(initial_bytes, initial_chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge(merge_pair, pair_frequences, words_by_tokens):\n",
        "    new_token = ''.join(merge_pair)\n",
        "    new_words_by_tokens = Counter()\n",
        "    \n",
        "    for word, freq in words_by_tokens.items():\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and (word[i], word[i + 1]) == merge_pair:\n",
        "                new_word.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_words_by_tokens[tuple(new_word)] = freq\n",
        "    \n",
        "    pair_frequences = Counter()\n",
        "    for word, freq in new_words_by_tokens.items():\n",
        "        for i in range(len(word) - 1):\n",
        "            pair_frequences[(word[i], word[i + 1])] += freq\n",
        "    \n",
        "    return pair_frequences, new_words_by_tokens\n",
        "\n",
        "def train(data, vocab_size=1024, special_tokens=None):\n",
        "    if vocab_size < 256:\n",
        "        raise ValueError(\"Vocab size can't be less than 256\")\n",
        "    if special_tokens is None:\n",
        "        special_tokens = []\n",
        "    \n",
        "    id2token = bytes_to_unicode()  \n",
        "    merges = []\n",
        "    \n",
        "    words_by_tokens = Counter()\n",
        "    for sample in tqdm(data, desc=\"Loading data\"):\n",
        "        words = WHITESPACE_SPLITTER.findall(sample.strip())\n",
        "        for word in words:\n",
        "            words_by_tokens[tuple(word)] += 1\n",
        "    \n",
        "    pair_frequences = Counter()\n",
        "    for word, freq in words_by_tokens.items():\n",
        "        for i in range(len(word) - 1):\n",
        "            pair_frequences[(word[i], word[i + 1])] += freq\n",
        "    \n",
        "    pbar = trange(vocab_size, desc=\"Building vocabulary\", initial=len(id2token) + len(special_tokens))\n",
        "    while len(id2token) < vocab_size - len(special_tokens):\n",
        "        if not pair_frequences:\n",
        "            print(\"Not enough data to fulfil vocabulary\")\n",
        "            break\n",
        "        \n",
        "        top_pair = max(pair_frequences, key=pair_frequences.get)\n",
        "        new_token = ''.join(top_pair)\n",
        "        del pair_frequences[top_pair]\n",
        "        \n",
        "        if new_token in id2token.values():\n",
        "            continue\n",
        "        \n",
        "        id2token[len(id2token)] = new_token\n",
        "        merges.append(top_pair)\n",
        "        \n",
        "        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)\n",
        "        pbar.update()\n",
        "    pbar.close()\n",
        "    \n",
        "    for special_token in special_tokens:\n",
        "        id2token[len(id2token)] = special_token\n",
        "    \n",
        "    return {v: k for k, v in id2token.items()}, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLwur-KgK990"
      },
      "outputs": [],
      "source": [
        "# Обучаем токенизатор на тренировочных текстах\n",
        "# Для нашей задачи хватит и небольшого словаря, но можете пробовать и большего размера обучить!\n",
        "\n",
        "\n",
        "vocab, merges = train(dataset[\"train\"][\"text\"], vocab_size=1024, special_tokens=[\"[EOS]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsapJUAK990"
      },
      "outputs": [],
      "source": [
        "# Посмотрим на случайные токены\n",
        "\n",
        "random_tokens = [512, 614, 768, 888, 1022]\n",
        "unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}\n",
        "for token_id in random_tokens:\n",
        "    token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "    raw_bytes = bytes([unicode_to_bytes[it] for it in token])\n",
        "    print(f\"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUz7cma1Czs"
      },
      "outputs": [],
      "source": [
        "class ByteLevelBPETokenizer:\n",
        "    def __init__(self, vocab, merges, eos_token=\"[EOS]\"):\n",
        "        super().__init__()\n",
        "        if eos_token not in vocab:\n",
        "            raise ValueError(\"There is no EOS token in vocab\")\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.token2id = vocab\n",
        "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
        "        self.eos_token = eos_token\n",
        "        self.eos_token_id = self.token2id[eos_token]\n",
        "        self.merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "    \n",
        "    @lru_cache\n",
        "    def bpe(self, word):\n",
        "        word = tuple(word)\n",
        "        pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "        while pairs:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i + 1]) == bigram:\n",
        "                    new_word.append(''.join(bigram))\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = tuple(new_word)\n",
        "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "        return word\n",
        "    \n",
        "    def encode(self, text, add_eos_token=True):\n",
        "        words = WHITESPACE_SPLITTER.findall(text)\n",
        "        tokens = [token for word in words for token in self.bpe(word)]\n",
        "        token_ids = [self.token2id[token] for token in tokens if token in self.token2id]\n",
        "        if add_eos_token:\n",
        "            token_ids.append(self.eos_token_id)\n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, idx):\n",
        "        tokens = [self.id2token[i] for i in idx if i in self.id2token]\n",
        "        text = ''.join(tokens)\n",
        "        return text\n",
        "    \n",
        "    \n",
        "    def push_to_hub(self, repo_id, *, private=None, token=None):\n",
        "        api = HfApi()\n",
        "        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id\n",
        "\n",
        "        # Push the files to the repo in a single commit\n",
        "        with SoftTemporaryDirectory() as tmp:\n",
        "            save_directory = Path(tmp) / repo_id\n",
        "            save_directory.mkdir(parents=True)\n",
        "            with open(save_directory / \"vocabulary.json\", \"w\") as f_out:\n",
        "                print(json.dumps(self.token2id, indent=2), file=f_out)\n",
        "            with open(save_directory / \"merges.json\", \"w\") as f_out:\n",
        "                print(json.dumps({\"merges\": self.merges}), file=f_out)\n",
        "\n",
        "            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):\n",
        "        if not os.path.isdir(pretrained_model_name_or_path):\n",
        "            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)\n",
        "        else:\n",
        "            storage_folder = pretrained_model_name_or_path\n",
        "        storage_folder = Path(storage_folder)\n",
        "        with open(storage_folder / \"vocabulary.json\", \"r\") as f_in:\n",
        "            vocab = json.load(f_in)\n",
        "        with open(storage_folder / \"merges.json\", \"r\") as f_in:\n",
        "            merges = [tuple(it) for it in json.load(f_in)[\"merges\"]]\n",
        "        return cls(vocab, merges, **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTQzO3wK991"
      },
      "outputs": [],
      "source": [
        "# Инициализируем токенизатор\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(vocab, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9m_65vBK991"
      },
      "outputs": [],
      "source": [
        "# Загружаем токенизатор на хаб\n",
        "\n",
        "tokenizer.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9VohtKrK991"
      },
      "outputs": [],
      "source": [
        "# Скачиваем токенизатор с хаба\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvf_Q8knPW4"
      },
      "outputs": [],
      "source": [
        "# Смотрим на работу токенизатора\n",
        "\n",
        "text = \"Что было полгода назад? Помимо грандиозных событий, полгода назад были ещё семинары по линейной алгебре.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "reverse_text = [tokenizer.decode([it]) for it in ids]\n",
        "print(\"|\".join(reverse_text))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgpwFYiK991"
      },
      "outputs": [],
      "source": [
        "# Посчитаем немного статистики по токенизации, определимся с размером контекста у модели\n",
        "\n",
        "lens = []\n",
        "for text in tqdm(dataset[\"test\"][\"text\"]):\n",
        "    ids = tokenizer.encode(text)\n",
        "    lens.append(len(ids))\n",
        "\n",
        "print(f\"Average token len per sample: {sum(lens) / len(lens):.2f}\")\n",
        "print(f\"Minimum and maximum lens are: {min(lens)} and {max(lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwN3mfmznPW5"
      },
      "source": [
        "Должно получиться в среднем по 70 токенов на последовательность.\n",
        "Контекста в 128 токенов будет вполне достаточно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1FljGXC7I-"
      },
      "source": [
        "# Модель [10 баллов]\n",
        "\n",
        "В качестве модели реализуем трансформер, в котором\n",
        "1. В качестве позиционных эмбеддингов используется ALiBi\n",
        "2. Механизм внимания использует GQA\n",
        "3. В Feed-Forward блоке SwiGLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZrCvkmK992"
      },
      "outputs": [],
      "source": [
        "# Для удобства заведем конфиг для модели\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_kv_head: int\n",
        "    hidden_dim: int\n",
        "    intermediate_dim: int\n",
        "    dropout: float = 0.1\n",
        "    vocab_size: int = 1024\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"nano\": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),\n",
        "    \"mini\": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),\n",
        "    \"small\": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYXMb5PEDFkt"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        norm_x = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        return self.scale * norm_x\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.head_dim = config.hidden_dim // config.n_head\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_per_kv = config.n_head // config.n_kv_head\n",
        "\n",
        "        self.q_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.kv_proj = nn.Linear(config.hidden_dim, 2 * config.n_kv_head * self.head_dim)\n",
        "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", self._create_causal_mask(config.max_seq_len))\n",
        "        self.register_buffer(\"alibi\", self._build_alibi_bias(config.n_head))\n",
        "\n",
        "    def _build_alibi_bias(self, num_heads: int) -> Tensor:\n",
        "        slopes = torch.tensor([2**(-8 * i / num_heads) for i in range(num_heads)])\n",
        "        return slopes.view(1, num_heads, 1, 1)\n",
        "\n",
        "    def _create_causal_mask(self, max_seq_len: int) -> Tensor:\n",
        "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
        "        return mask.view(1, 1, max_seq_len, max_seq_len)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.config.n_head, self.head_dim).transpose(1, 2)\n",
        "        kv = self.kv_proj(x).view(batch_size, seq_len, 2, self.config.n_kv_head, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "        k = k.repeat_interleave(self.q_per_kv, dim=1)  # приведение k, v к одной размерности\n",
        "        v = v.repeat_interleave(self.q_per_kv, dim=1)\n",
        "        \n",
        "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale + self.alibi\n",
        "        attn_weights = attn_weights.masked_fill(self.causal_mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        output = (attn_probs @ v).transpose(1, 2).reshape(batch_size, seq_len, self.config.hidden_dim)\n",
        "        return self.out_proj(output)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config.hidden_dim, config.hidden_dim * 2)\n",
        "        self.fc2 = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x1, x2 = self.fc1(x).chunk(2, dim=-1)\n",
        "        return self.fc2(F.silu(x1) * x2)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        x = x + self.res_dropout_1(self.attn(self.ln_1(x), attention_mask))\n",
        "        x = x + self.res_dropout_2(self.mlp(self.ln_2(x)))\n",
        "        return x\n",
        "\n",
        "class TransformerForCausalLM(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        x = self.token_emb(input_ids)\n",
        "        x = self.emb_dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        x = self.ln_final(x)\n",
        "        return self.lm_head(x)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None) -> Tensor:\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
        "            logits = self(idx_cond)[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                top_values, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < top_values[:, -1, None]] = float('-inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) if do_sample else torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdavMmSDujw"
      },
      "source": [
        "# Train Loop [2 + 2 балла]\n",
        "\n",
        "Настало время обучать модель.\n",
        "Небольшую можно пробовать обучать локально, но лучше всего воспользоваться GPU, например, на Google Colab.\n",
        "\n",
        "За реализацию 2 балла, и еще 2 балла - если модель научилась генерить анекдоты.\n",
        "\n",
        "Не забудьте проверить, что вы загрузили нужные веса на HF и у проверяющего скачается нужная версия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wtAiR_aDxed"
      },
      "outputs": [],
      "source": [
        "# Определим датасет и как заворачивать семплы в батч\n",
        "# Разные тексты имеют разную длину, поэтому будет падить до самого длина семпла\n",
        "# Так же заведем дополнительную маску, чтобы механизм внимания не учитывал падинги\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.texts[idx]\n",
        "        tokenized_sequence = self.tokenizer.encode(texts)\n",
        "        return tokenized_sequence\n",
        "\n",
        "\n",
        "def data_collator(\n",
        "    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    batch_size = len(tokenized_sequences)\n",
        "    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)\n",
        "    attention_mask = torch.zeros((batch_size, max_batch_seq_len))\n",
        "\n",
        "    for i, tok_seq in enumerate(tokenized_sequences):\n",
        "        cur_len = min(len(tok_seq), max_batch_seq_len)\n",
        "        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])\n",
        "        attention_mask[i, :cur_len] = 1\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):\n",
        "    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "_d = TextDataset([\"Привет!\", \"Как твои дела?\", \"Осталось совсем немного до конца\"], tokenizer)\n",
        "_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)\n",
        "\n",
        "for i, batch in enumerate(_dl):\n",
        "    print(f\"Batch #{i}\")\n",
        "    input_ids, attn_mask = batch\n",
        "    print(input_ids, attn_mask, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i719AOdQK993"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torch import Tensor\n",
        "\n",
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Scheduler for Optimizer with linear warmup and linear decay to the end of training\n",
        "\n",
        "    Args:\n",
        "        optimizer: torch optimizer to control learning rate\n",
        "        num_warmup_steps: number of warmup steps\n",
        "        num_training_steps: total number of training steps\n",
        "    Return:\n",
        "        torch learning rate scheduler\n",
        "    \"\"\"\n",
        "    assert num_training_steps >= num_warmup_steps\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:\n",
        "    \"\"\"Calculate Cross-Entropy loss for Language Modeling task\n",
        "    Under the hood:\n",
        "    1. Create targets based on input ids\n",
        "    2. Masked out tokens corresponded to paddings\n",
        "    3. Calculate cross entropy loss\n",
        "\n",
        "    Args:\n",
        "        input_ids: tensor with input ids, shape [bs, seq len]\n",
        "        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]\n",
        "        logits: predicted logits, shape [bs, seq len, vocab size]\n",
        "    Return:\n",
        "        cross entropy loss, single-item tensor\n",
        "    \"\"\"\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = input_ids[:, 1:].contiguous()\n",
        "    shift_mask = attention_mask[:, 1:].contiguous()\n",
        "    \n",
        "    loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    loss = loss.view(shift_labels.size())\n",
        "    loss = loss * shift_mask\n",
        "    return loss.sum() / shift_mask.sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYdF52zXtoX"
      },
      "outputs": [],
      "source": [
        "# Определим тренера с наиболее важными гиперпараметрами для обучения\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        clip_grad_norm=1.0,\n",
        "        n_steps=10_000,\n",
        "        val_every_n_steps=1_000,\n",
        "        plot_every_n_steps=100,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.n_steps = n_steps\n",
        "        self.val_every_n_steps = val_every_n_steps\n",
        "        self.plot_every_n_steps = plot_every_n_steps\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, val_loader):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def run(self, model, train_loader, val_loader):\n",
        "        model = model.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "        plotlosses = PlotLosses(figsize=(15, 9), step_names=\"Step\")\n",
        "        logs = {\"lr\": 0, \"epoch\": 0}\n",
        "\n",
        "        data_iter = iter(train_loader)\n",
        "        for iter_num in range(self.n_steps):\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                logs[\"epoch\"] += 1\n",
        "                batch = next(data_iter)\n",
        "\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            loss = cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:\n",
        "                val_loss = self.validate(model, val_loader)\n",
        "                plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % self.plot_every_n_steps == 0:\n",
        "                logs[\"loss\"] = loss.item()\n",
        "                logs[\"lr\"] = scheduler.get_last_lr()[0]\n",
        "                plotlosses.update(logs, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "\n",
        "        val_loss = self.validate(model, val_loader)\n",
        "        plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "        plotlosses.send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1BpJflMTAi"
      },
      "outputs": [],
      "source": [
        "# Создаем тренировочный и тестовые даталоадеры\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = TextDataset(dataset[\"train\"][\"text\"], tokenizer)\n",
        "train_dataloader = create_dataloader(\n",
        "    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(dataset[\"test\"][\"text\"], tokenizer)\n",
        "test_dataloader = create_dataloader(\n",
        "    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jHSgMZECGl"
      },
      "outputs": [],
      "source": [
        "# Инициализируем модель\n",
        "\n",
        "config = model_configs[\"nano\"]\n",
        "model = TransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUsjHl4Nkoa"
      },
      "outputs": [],
      "source": [
        "# Инициализируем тренера\n",
        "\n",
        "trainer = Trainer(learning_rate=9e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUgUfpKK993"
      },
      "outputs": [],
      "source": [
        "# Обучение goes brrrr!\n",
        "\n",
        "trainer.run(model, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94zNDLEqdQow"
      },
      "outputs": [],
      "source": [
        "# Смотрим на качество генерации глазами\n",
        "# Для маленьких и слабых моделей \"затягиваем\" гайки генерации\n",
        "\n",
        "text = \"Заходит в бар\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]\n",
        "print(input_ids)\n",
        "model_output = model.generate(\n",
        "    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFi7M9ExHWv9"
      },
      "outputs": [],
      "source": [
        "# Загружаем модель на хаб\n",
        "\n",
        "model.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrMwV9zK993"
      },
      "source": [
        "Поиграйтесь с гиперпараметрами, попробуйте обучить `mini` и `small` версии.\n",
        "Постарайтесь добиться как можно более высокого качества как в терминах лосса, так и при визуальной оценке генерации.\n",
        "\n",
        "### Дополнительные баллы\n",
        "\n",
        "Вы также можно заработать дополнительные баллы:\n",
        "- Реализовать Rotary Positional Embedding **[4 балла]**\n",
        "- Реализовать Multi-Head Latent Attention **[2 балл]**\n",
        "- Оформить репозиторий на 🤗: карточка модели с описанием задания, репортом качества и примерами генерации **[2 балл]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8bIDy5dKXM"
      },
      "source": [
        "# Специальный раздел для проверяющего"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZplshN5HtRb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)\n",
        "check_model = TransformerForCausalLM.from_pretrained(REPO_NAME)\n",
        "check_model = check_model.to(device)\n",
        "check_model = check_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araF_3noK994"
      },
      "outputs": [],
      "source": [
        "text = \"Штирлиц пришел домой\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text), device=device)\n",
        "model_output = check_model.generate(\n",
        "    input_ids[None, :], max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
